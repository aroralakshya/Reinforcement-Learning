{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import cv2\n",
    "import time\n",
    "from random import randint\n",
    "import random\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import math\n",
    "from collections import deque\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Input\n",
    "from keras.optimizers import Adam\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_size, action_size, env):\n",
    "        self.env = env\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.maxScore = 0\n",
    "        \n",
    "        self.memory = deque(maxlen=50000)\n",
    "        \n",
    "        self.gam = 0.99 # discount\n",
    "        self.eps = 1.0 # exploration rate\n",
    "        self.eps_decay = .993\n",
    "        self.eps_min = 0.01\n",
    "        self.learning_rate = 0.001\n",
    "        \n",
    "        self.action_space = [*range(self.action_size)]\n",
    "        \n",
    "        self.model = self.build_model()\n",
    "        self.select_model = self.build_model()\n",
    "        \n",
    "        self.batch_size=32\n",
    "        \n",
    "    def build_model(self):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "    \n",
    "        return model\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done, score):\n",
    "        self.memory.append((state, action, reward, next_state, done, score))\n",
    "        \n",
    "    def rememberList(self, memoList):\n",
    "        self.memory.extend(memoList)\n",
    "        \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.eps:\n",
    "            return env.action_space.sample()\n",
    "        state = np.array([state])\n",
    "        action_val = self.model.predict(state)\n",
    "        return np.argmax(action_val[0])\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        \n",
    "        states = np.array([i[0] for i in minibatch])\n",
    "        \n",
    "        rewards = np.array([i[2] for i in minibatch])\n",
    "        \n",
    "        actions = np.array([i[1] for i in minibatch])\n",
    "\n",
    "        next_states = np.array([i[3] for i in minibatch])\n",
    "        \n",
    "        done = np.array([i[4] for i in minibatch])\n",
    "        \n",
    "        ind = np.array([i for i in range(self.batch_size)])\n",
    "        \n",
    "        next_state_select = self.select_model.predict_on_batch(next_states)\n",
    "        \n",
    "        action_select = np.argmax(next_state_select , axis=1)\n",
    "        \n",
    "        next_state_eval = self.model.predict_on_batch(next_states)\n",
    "        \n",
    "        action_eval = next_state_eval[[ind], [action_select]]\n",
    "        \n",
    "        targets = (rewards + self.gam * action_eval*(1-done))\n",
    "        \n",
    "        values_current = self.model.predict_on_batch(states)\n",
    "        \n",
    "        values_current[[ind], [actions]] = targets\n",
    "        \n",
    "        self.model.fit(states, values_current, epochs=1, verbose=0)\n",
    "        \n",
    "        if len(self.memory) % 200 == 0:\n",
    "            self.update_model()\n",
    "        \n",
    "    def decay_eps(self):\n",
    "        self.eps = max(self.eps_min, self.eps * self.eps_decay)\n",
    "    \n",
    "    def update_model(self):\n",
    "        self.select_model.set_weights(self.model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "env = gym.make('LunarLander-v2')\n",
    "agent = Agent(8,4, env)\n",
    "score_history=[]\n",
    "num_epis = 6000\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "for i in range(1, num_epis+1):\n",
    "    done = 0\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = agent.act(state)\n",
    "        \n",
    "        next_state, reward, done, inf = env.step(action)\n",
    "        agent.memory.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        agent.replay()\n",
    "    agent.eps = max(agent.eps * agent.eps_decay, agent.eps_min) \n",
    "\n",
    "    score_history += [score]\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "    print('episode %d, score %.2f, avg score %.2f, eps %.3f' % (i, score, avg_score, agent.eps))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
